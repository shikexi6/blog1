[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\nCoco was here."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Demystifying Machine Learning Models: A Beginner’s Guide to Decision Trees, KNN, SVMs, Naive Bayes, and Linear Models",
    "section": "",
    "text": "Machine Learning. Team (2023)\nWith a prevalence of machine learning, more and more industries and schools have made it as a required course to assist in data-driven decisions and predictions. It aims to develop and study statistical algorithm by learning from previous data and making generalization to unseen data. However, depending on different types of data, there are so many models available and choosing the right one has became the top concerns that people may have. In this blog, I explained 3 popular machine learning models — Decision Trees, K-Nearest Neighbors (KNN), Naive Bayes — to explain how they work, their pros and cons, and when to use them. This post is intended for the people has some knowledge about machine learning but don’t know how to choose among different models. By the end, you’ll have a clear understanding of these foundational techniques and their applications."
  },
  {
    "objectID": "posts/post-with-code/index.html#when-to-use-decision-trees",
    "href": "posts/post-with-code/index.html#when-to-use-decision-trees",
    "title": "Demystifying Machine Learning Models: A Beginner’s Guide to Decision Trees, KNN, SVMs, Naive Bayes, and Linear Models",
    "section": "When To Use Decision Trees?",
    "text": "When To Use Decision Trees?\nDecision Trees are versatile and widely used in machine learning for both classification and regression tasks.\n\nPros:\n\nHighly Interpretable since it is very similar to the process of human making decisions.\nHandle irrelevant features by splitting only on the most important ones.\nRank features based on their importance, providing insights into which features drive predictions.\nIt can handle both numerical and categorical data.\n\n\n\nCons:\n\nIt is easy to overfitting on noisy data, so it is importance to control its tree depth, which is the length of the longest path from the tree root to a leaf.\nSensitive to small variations in the data: If two features have nearly equal importance, a slight change in the dataset might lead the tree to split on one feature instead of the other, altering downstream splits and the tree’s predictions."
  },
  {
    "objectID": "posts/post-with-code/index.html#example",
    "href": "posts/post-with-code/index.html#example",
    "title": "Demystifying Machine Learning Models: A Beginner’s Guide to Decision Trees, KNN, SVMs, Naive Bayes, and Linear Models",
    "section": "Example:",
    "text": "Example:\nImagine you have three different job offers with comparable salaries and job descriptions. In this case, you want to decide which one to accept, and make this decision based on which job is likely to make you happy. There are three things that is associated with your happiness: supportive colleagues, work-hour flexibility, whether the company is a start-up or not. Thus, a decision tree may be used for this case.\nThe dataframe looks like this with 1 for yes and 0 for no:\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nwork_hour_flexibility\nstart_up\n\n\n\n\n0\n1\n0\n0\n\n\n1\n0\n0\n1\n\n\n2\n0\n1\n1\n\n\n3\n1\n1\n1\n\n\n\n\n\n\n\nYou also get data from toy survey. You decide to train a machine learning model using this toy survey data and use this model to predict which job from offer_df is likely to make you happy.\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nwork_hour_flexibility\nstart_up\ntarget\n\n\n\n\n0\n1\n1\n1\nhappy\n\n\n1\n1\n1\n0\nhappy\n\n\n2\n1\n0\n1\nhappy\n\n\n3\n0\n1\n0\nunhappy\n\n\n4\n0\n1\n1\nunhappy\n\n\n5\n1\n0\n0\nhappy\n\n\n6\n1\n1\n0\nhappy\n\n\n7\n0\n0\n1\nunhappy\n\n\n8\n1\n0\n1\nunhappy\n\n\n9\n0\n0\n0\nunhappy\n\n\n\n\n\n\n\nTherefore, you can follow the simple steps below to train the model and make decisions 1. Create a decision tree classifier object, where random_state here is used to ensure the randomness is consistent every time you run the code.\n\nfrom sklearn.tree import DecisionTreeClassifier\ntoy_tree = DecisionTreeClassifier(random_state=16)\n\n\nFit the decision tree classifier with toy dataframe.\n\n\ntoy_tree.fit(X_train_toy, y_train_toy)\n\nDecisionTreeClassifier(random_state=16)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(random_state=16) \n\n\n\nVisualize the trained decision tree.\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nwidth = 12\nheight = 8\nclass_names = [\"happy\", \"unhappy\"]\nfeature_names = X_train_toy.columns.to_list()\nplt.figure(figsize=(width, height))\ntoy_tree_viz = tree.plot_tree(toy_tree,\n                              feature_names = feature_names,\n                              class_names = class_names,\n                              impurity=False,\n                              fontsize=10,\n                              filled=True)\ntoy_tree_viz\n\n[Text(0.3333333333333333, 0.875, 'supportive_colleagues &lt;= 0.5\\nsamples = 10\\nvalue = [5, 5]\\nclass = happy'),\n Text(0.16666666666666666, 0.625, 'samples = 4\\nvalue = [0, 4]\\nclass = unhappy'),\n Text(0.25, 0.75, 'True  '),\n Text(0.5, 0.625, 'start_up &lt;= 0.5\\nsamples = 6\\nvalue = [5, 1]\\nclass = happy'),\n Text(0.41666666666666663, 0.75, '  False'),\n Text(0.3333333333333333, 0.375, 'samples = 3\\nvalue = [3, 0]\\nclass = happy'),\n Text(0.6666666666666666, 0.375, 'work_hour_flexibility &lt;= 0.5\\nsamples = 3\\nvalue = [2, 1]\\nclass = happy'),\n Text(0.5, 0.125, 'samples = 2\\nvalue = [1, 1]\\nclass = happy'),\n Text(0.8333333333333334, 0.125, 'samples = 1\\nvalue = [1, 0]\\nclass = happy')]\n\n\n\n\n\n\n\n\n\nThe root node splits on supportive_colleagues, which means that if the colleagues is not supportive (values less than or equal to 0.5), it will lead to unhappy. However, if the colleagues are supportive (supportive_colleagues &gt; 0.5), it will lead to the next question of if the company is a start up. If the company is not a start up (start_up &lt;= 0.5), it will be happy, while for start up (&gt; 0.5), the tree further splits on work_hour_flexibility. In this branch, if the work_hour_flexibility &lt;= 0.5, the majority class is happy, but if work_hour_flexibility &gt; 0.5, the classification remains happy.\n\nWith the trained decision tree, we now make prediction on our offers.\n\n\npredictions = toy_tree.predict(offer_df).tolist()\n\nhappy_job_indices = [index for index, prediction in enumerate(predictions) if prediction == \"happy\"]\nhappy_job_indices\n\n[0, 3]\n\n\nTherefore, we are happy about the offer with indices of 0 and 3, so we can choose offer from these two.\nI believe now you have a basic understanding of decision tree. However, this is just a simple example. In real time, the dataset is larger and noisy. The decision tree requires more careful control on its tree depth with selecting the features."
  },
  {
    "objectID": "posts/post-with-code/index.html#when-to-use-knn",
    "href": "posts/post-with-code/index.html#when-to-use-knn",
    "title": "Demystifying Machine Learning Models: A Beginner’s Guide to Decision Trees, KNN, SVMs, Naive Bayes, and Linear Models",
    "section": "When To Use KNN?",
    "text": "When To Use KNN?\nKNN is a great starting point for classification and regression tasks when simplicity, interpretability, and relatively small data size are priorities.\n\nPros:\n\nIt is easy to interpret since its logic is kind straightforward that people are grouped by their flock and a new person you met will likely have similar likes as his nearest neighbours. So as data point, a new point will likely have the same label/value as its nearest neighbors.\nKNN captures local structure in the data well: If the dataset has segments or groups that differ significantly, KNN can handle that naturally by focusing on local neighborhoods.\nIt works for both classification and regression with minimal changes (mostly how you aggregate the neighbors’ information—majority vote for classification, average for regression).\n\n\n\nCons:\n\nIt is not suitable for large dataset: Since KNN has no explicit training phase, and learn from the entire dataset, compute distances to all training points to find the nearest neighbors, It can be computationally Expensive at Prediction\nSimilar to decision tree, it also needs to control for k, which is the number of neighbors to consider. A small k is susceptible to noise and outliers, while a large k can dilute local patterns.\nStruggles with High Dimensions: If there are many features, the notion of “distance” can become less meaningful since most points end up similarly distant. Therefore, you may need to reduce dimensionality first.\nMust Scale/Normalize Data: Since KNN is based on calculating the distance, if different features have widely varying scales, distance measures might be dominated by features with large magnitude."
  },
  {
    "objectID": "posts/post-with-code/index.html#example-1",
    "href": "posts/post-with-code/index.html#example-1",
    "title": "Demystifying Machine Learning Models: A Beginner’s Guide to Decision Trees, KNN, SVMs, Naive Bayes, and Linear Models",
    "section": "Example:",
    "text": "Example:\nImagine you have a small dataset of flowers with the following features of Sepal length, Sepal width, Petal length, Petal width. Each flower is labeled as one of three species: setosa, versicolor, or virginica. By training on the KNN, you want to predict which of the tree species belongs to when you have a new flower’s measurements.\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n2\n\n\n146\n6.3\n2.5\n5.0\n1.9\n2\n\n\n147\n6.5\n3.0\n5.2\n2.0\n2\n\n\n148\n6.2\n3.4\n5.4\n2.3\n2\n\n\n149\n5.9\n3.0\n5.1\n1.8\n2\n\n\n\n\n150 rows × 5 columns\n\n\n\nTherefore, here are some example code: 1. Create and train the KNN classifier, here we will use k=3 neighbours\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\n\nPredict on a new sample (for example: sepal length=5.0, sepal width=3.0, petal length=1.6, petal width=0.4)\n\n\nnew_sample = [[5.0, 3.0, 1.6, 0.4]]\npredicted_class = knn.predict(new_sample)\nprint(\"Predicted species (as name):\", iris.target_names[predicted_class][0])\n\nPredicted species (as name): setosa\n\n\nTherefore, the predicted species is setosa on our new sample."
  },
  {
    "objectID": "posts/post-with-code/index.html#pros-2",
    "href": "posts/post-with-code/index.html#pros-2",
    "title": "Demystifying Machine Learning Models: A Beginner’s Guide to Decision Trees, KNN, SVMs, Naive Bayes, and Linear Models",
    "section": "Pros",
    "text": "Pros\n\nIt is fast and efficient because of itsclosed-form calculations rather than iterative optimization. Many machine learning algorithms (e.g., logistic regression, neural networks) use iterative procedures like gradient descent, which can be time-consuming for large datasets. However, Naive Bayes trains by calculating straightforward probability estimates (mean, variance for each feature, or frequency counts).\nIt works well with Noisy Data: Although the independence assumption is hard to follow, it still works well with Noisy data. Since Naive Bayes treats each feature’s contribution to the class independently, the “correct” features can still overwhelmingly point to the right class, effectively “averaging out” the noise.\nIt works well with High-Dimensional Data (text): Similar to the reason above, Naive Bayes is known to produce strong baseline results by ignoring inter-feature dependencies that might otherwise exacerbate noise issues."
  },
  {
    "objectID": "posts/post-with-code/index.html#cons-2",
    "href": "posts/post-with-code/index.html#cons-2",
    "title": "Demystifying Machine Learning Models: A Beginner’s Guide to Decision Trees, KNN, SVMs, Naive Bayes, and Linear Models",
    "section": "Cons",
    "text": "Cons\n\nAssumption of Independence brings benefits but also brings negative side to the model, since the assumption is hard to be 100% true in practice\nIf a class–feature combination isn’t observed in the training set, it will assign zero probability unless you use smoothing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Demystifying Machine Learning Models: A Beginner’s Guide to Decision Trees, KNN, SVMs, Naive Bayes, and Linear Models\n\n\n\n\n\n\nComparison and Contrasts\n\n\nExplanations\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nCoco Shi\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]